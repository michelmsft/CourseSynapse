{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### SAS Access Token Configuration\r\n",
        "\r\n",
        "this initial code sets up the necessary configurations for accessing data in your Azure Data Lake container. By defining the **storage account name**, **container name**, and **SAS token**, and then setting the Spark configuration, you ensure that your Spark session can securely connect to the Data Lake. This setup is a preliminary step before diving into more substantial data exploration and analysis tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Base Parameters\r\n",
        "\r\n",
        "storage_account_name    = \"dcadlsgen2\"\r\n",
        "container_name          = \"data\"\r\n",
        "sas_token               = \"sp=rwl&st=2024-10-30T20:59:32Z&se=2024-11-06T05:59:32Z&skoid=76091599-8241-44cb-aa8e-18d7e8b9cf37&sktid=304616a7-7248-4620-8cee-5562f30ee9ff&skt=2024-10-30T20:59:32Z&ske=2024-11-06T05:59:32Z&sks=b&skv=2022-11-02&spr=https&sv=2022-11-02&sr=c&sig=LfEQAtCwCbYsxM8L%2F%2BrVJaMQRI4%2FLnAWEO%2FRn%2FhbjQE%3D\"\r\n",
        "\r\n",
        "# Set Spark configuration to use the SAS token\r\n",
        "\r\n",
        "spark.conf.set(f\"fs.azure.sas.{container_name}.{storage_account_name}.dfs.core.windows.net\", sas_token)\r\n",
        "\r\n",
        "# Define the base path for the Data Lake container\r\n",
        "\r\n",
        "base_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Creating Dataframes\r\n",
        "in this section, we are loading patient, doctor, appointment, and medical record data from CSV files into Spark DataFrames, with headers and schema inferred from the data. It also introduces the concept of lazy evaluation in Spark, where transformations are not executed until an action is called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create a Patients, Doctors, Appointments and MedicalRecords Dataframes\r\n",
        "\r\n",
        "# Define file paths for each dataset\r\n",
        "\r\n",
        "patients_file_path      =    f\"{base_path}ehr/bronz/*/Patients.csv\"\r\n",
        "doctors_file_path       =    f\"{base_path}ehr/bronz/*/Doctors.csv\"\r\n",
        "appointments_file_path  =    f\"{base_path}ehr/bronz/*/Appointments.csv\"\r\n",
        "medrecords_file_path    =    f\"{base_path}ehr/bronz/*/MedicalRecords.csv\"\r\n",
        "\r\n",
        "# Load the CSV files into Spark DataFrames with headers and inferred schema\r\n",
        "\r\n",
        "df_patients             =   spark.read.csv(patients_file_path, header=True, inferSchema=True)\r\n",
        "df_doctors              =   spark.read.csv(doctors_file_path, header=True, inferSchema=True)\r\n",
        "df_appointments         =   spark.read.csv(appointments_file_path, header=True, inferSchema=True)\r\n",
        "df_medrecords           =   spark.read.csv(medrecords_file_path, header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Lazy evaluation and action \r\n",
        "```count()``` is an example of an action command that triggers the execution of transformations. In this case, it is an aggregation function. This action command will count the number of rows in the DataFrame, causing Spark to execute the read operations. The preceding code lines are preparatory commands, setting up the necessary configurations and transformations before any action is taken.\r\n",
        "\r\n",
        "I love to think of Lazy evaluation as a **“preparatory command”** to describe the setup steps before the action is executed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\r\n",
        "\r\n",
        "# Count the number of records in each DataFrame and print the results\r\n",
        "\r\n",
        "print(f\"Number of records in Patients DataFrame: {df_patients.count():,.2f}\")\r\n",
        "print(f\"Number of records in Doctors DataFrame: {df_doctors.count():,.2f}\")\r\n",
        "print(f\"Number of records in Appointments DataFrame: {df_appointments.count():,.2f}\")\r\n",
        "print(f\"Number of records in Medical Records DataFrame: {df_medrecords.count():,.2f}\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Using the ```show()``` Function\r\n",
        "The show() function is another action command in Spark. It displays the top rows of a DataFrame, providing a quick way to inspect the data. Using show() with a specified number of rows helps you verify that the data has been loaded correctly and gives you a snapshot of the DataFrame’s contents without overwhelming you with too much information.For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_patients.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### ```Display()``` function\r\n",
        "**The display function is a tool used to render a DataFrame** in a tabular format, showing rows and columns similar to a spreadsheet. **display** is **not** an official method of the DataFrame API. It is more of a function provided by specific interactive environments. To limit the number of rows displayed when using the display() function in a Spark notebook, you can use the limit() method before calling display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Limit the number of rows to 10 and display the Patients DataFrame\r\n",
        "\r\n",
        "display(df_patients.limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The ```display(df_patients, summary=True)``` command in a Spark notebook is used to display a summary of the df_patients DataFrame. When you set summary=True, it provides a concise overview of the DataFrame, including statistics such as the count, mean, standard deviation, minimum, and maximum values for each column. This is particularly useful for quickly understanding the distribution and characteristics of your data without displaying all the rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(df_patients,summary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Select Columns\r\n",
        "\r\n",
        "**select** method in the DataFrame API is a **transformation** function. It allows you to create a new DataFrame by selecting specific columns or expressions from an existing DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_patients_vw = df_patients.select(\"PatientID\",\"Firstname\",\"LastName\",\"Gender\")\r\n",
        "df_patients_vw.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Transform data with Apache Spark in Azure Synapse Analytics\r\n",
        "\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "df_patients_ga_vw = df_patients.filter(df_patients.Source==\"GA\")\r\n",
        "display(df_patients_ga_vw,summary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "from pyspark.sql.functions import col, current_date, datediff, floor\r\n",
        "\r\n",
        "# Add a new column 'Age' to the DataFrame\r\n",
        "df_patients_w_age_vw = df_patients.withColumn(\r\n",
        "    \"Age\",\r\n",
        "    floor(datediff(current_date(), col(\"DateOfBirth\")) / 365.25)\r\n",
        ")\r\n",
        "\r\n",
        "# Display the updated DataFrame with the new 'Age' column\r\n",
        "display(df_patients_w_age_vw.limit(5))\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create a table in the metastore for the patients with the calculated age\r\n",
        "df_patients_w_age_vw.createOrReplaceTempView(\"t_patients\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Usig Spark SQL to query data using SQL\r\n",
        "\r\n",
        "This code snippet demonstrates how to use Spark SQL to filter and display records of patients older than 60 years."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Filtering Elderly Patients and Displaying Results\r\n",
        "\r\n",
        "df_patients_eldery= spark.sql(\"SELECT PatientID, FirstName,LastName, Gender,Age, Source \\\r\n",
        "                      FROM t_patients \\\r\n",
        "                      WHERE Age>60\")\r\n",
        "display(df_patients_eldery,summary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Write the data from the temporary view to a permanent table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Write the data from the temporary view to a permanent table\r\n",
        "spark.sql(\"\"\"\r\n",
        "    CREATE TABLE IF NOT EXISTS EhrLakeDB.DimPatients_stg\r\n",
        "    AS SELECT * FROM t_patients\r\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Drop the table\r\n",
        "spark.sql(\"\"\"DROP TABLE IF EXISTS lakedb.DimPatients_stg\"\"\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "This code snippet uses a SQL magic command to query a sample of patient records from the `t_patients` table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        },
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "Select * from t_patients LIMIT 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "# Convert AppointmentDate to date format\r\n",
        "df_appointments = df_appointments.withColumn(\"AppointmentDate\", F.to_date(F.col(\"AppointmentDate\")))\r\n",
        "\r\n",
        "# Truncate AppointmentDate to the first day of each month before aggregation\r\n",
        "df_appointments = df_appointments.withColumn(\"Month\", F.date_trunc(\"month\", F.col(\"AppointmentDate\")))\r\n",
        "\r\n",
        "# Filter, group by Month, and count appointments for TX\r\n",
        "df_appointment_TX = df_appointments.filter(df_appointments.Source == \"TX\") \\\r\n",
        "    .groupBy(\"Month\") \\\r\n",
        "    .agg(F.count(\"AppointmentID\").alias(\"TotalAppointments\")) \\\r\n",
        "    .toPandas()\r\n",
        "\r\n",
        "# Filter, group by Month, and count appointments for GA\r\n",
        "df_appointment_GA = df_appointments.filter(df_appointments.Source == \"GA\") \\\r\n",
        "    .groupBy(\"Month\") \\\r\n",
        "    .agg(F.count(\"AppointmentID\").alias(\"TotalAppointments\")) \\\r\n",
        "    .toPandas()\r\n",
        "\r\n",
        "# Filter, group by Month, and count appointments for NY\r\n",
        "df_appointment_NY = df_appointments.filter(df_appointments.Source == \"NY\") \\\r\n",
        "    .groupBy(\"Month\") \\\r\n",
        "    .agg(F.count(\"AppointmentID\").alias(\"TotalAppointments\")) \\\r\n",
        "    .toPandas()\r\n",
        "\r\n",
        "# Convert Month to datetime and ensure it's sorted\r\n",
        "df_appointment_TX['Month'] = pd.to_datetime(df_appointment_TX['Month'])\r\n",
        "df_appointment_GA['Month'] = pd.to_datetime(df_appointment_GA['Month'])\r\n",
        "df_appointment_NY['Month'] = pd.to_datetime(df_appointment_NY['Month'])\r\n",
        "\r\n",
        "# Sort the data\r\n",
        "df_appointment_TX = df_appointment_TX.sort_values(by=\"Month\", ascending=True)\r\n",
        "df_appointment_GA = df_appointment_GA.sort_values(by=\"Month\", ascending=True)\r\n",
        "df_appointment_NY = df_appointment_NY.sort_values(by=\"Month\", ascending=True)\r\n",
        "\r\n",
        "# Define the date range for filtering\r\n",
        "start_date = pd.to_datetime('2024-01-01')\r\n",
        "end_date = pd.to_datetime('2024-10-30')\r\n",
        "\r\n",
        "# Filter the DataFrames for the specified date range\r\n",
        "df_appointment_TX = df_appointment_TX[(df_appointment_TX['Month'] >= start_date) & (df_appointment_TX['Month'] <= end_date)]\r\n",
        "df_appointment_GA = df_appointment_GA[(df_appointment_GA['Month'] >= start_date) & (df_appointment_GA['Month'] <= end_date)]\r\n",
        "df_appointment_NY = df_appointment_NY[(df_appointment_NY['Month'] >= start_date) & (df_appointment_NY['Month'] <= end_date)]\r\n",
        "\r\n",
        "# Create a simple line plot with different colors for each state\r\n",
        "plt.figure(figsize=(10, 5))\r\n",
        "\r\n",
        "plt.plot(df_appointment_TX['Month'], df_appointment_TX['TotalAppointments'], marker='o', color='blue', label='TX')\r\n",
        "plt.plot(df_appointment_GA['Month'], df_appointment_GA['TotalAppointments'], marker='o', color='green', label='GA')\r\n",
        "plt.plot(df_appointment_NY['Month'], df_appointment_NY['TotalAppointments'], marker='o', color='red', label='NY')\r\n",
        "\r\n",
        "plt.title('Appointments from January to October 2024')\r\n",
        "plt.xlabel('Month')\r\n",
        "plt.ylabel('# Appointments')\r\n",
        "plt.xticks(rotation=45)\r\n",
        "plt.legend()\r\n",
        "plt.tight_layout()\r\n",
        "plt.show()\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_appointments.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "# Convert AppointmentDate to date format\r\n",
        "df_appointments = df_appointments.withColumn(\"AppointmentDate\", F.to_date(F.col(\"AppointmentDate\")))\r\n",
        "\r\n",
        "# Truncate AppointmentDate to the first day of each month before aggregation\r\n",
        "df_appointments = df_appointments.withColumn(\"Month\", F.date_trunc(\"month\", F.col(\"AppointmentDate\")))\r\n",
        "\r\n",
        "# Filter, group by Month, and count appointments for TX\r\n",
        "df_appointment_TX = df_appointments.filter(df_appointments.Source == \"TX\") \\\r\n",
        "    .groupBy(\"Month\") \\\r\n",
        "    .agg(F.count(\"AppointmentID\").alias(\"TotalAppointments\")) \\\r\n",
        "    .toPandas()\r\n",
        "\r\n",
        "# Filter, group by Month, and count appointments for GA\r\n",
        "df_appointment_GA = df_appointments.filter(df_appointments.Source == \"GA\") \\\r\n",
        "    .groupBy(\"Month\") \\\r\n",
        "    .agg(F.count(\"AppointmentID\").alias(\"TotalAppointments\")) \\\r\n",
        "    .toPandas()\r\n",
        "\r\n",
        "# Filter, group by Month, and count appointments for NY\r\n",
        "df_appointment_NY = df_appointments.filter(df_appointments.Source == \"NY\") \\\r\n",
        "    .groupBy(\"Month\") \\\r\n",
        "    .agg(F.count(\"AppointmentID\").alias(\"TotalAppointments\")) \\\r\n",
        "    .toPandas()\r\n",
        "\r\n",
        "# Convert Month to datetime and ensure it's sorted\r\n",
        "df_appointment_TX['Month'] = pd.to_datetime(df_appointment_TX['Month'])\r\n",
        "df_appointment_GA['Month'] = pd.to_datetime(df_appointment_GA['Month'])\r\n",
        "df_appointment_NY['Month'] = pd.to_datetime(df_appointment_NY['Month'])\r\n",
        "\r\n",
        "# Sort the data\r\n",
        "df_appointment_TX = df_appointment_TX.sort_values(by=\"Month\", ascending=True)\r\n",
        "df_appointment_GA = df_appointment_GA.sort_values(by=\"Month\", ascending=True)\r\n",
        "df_appointment_NY = df_appointment_NY.sort_values(by=\"Month\", ascending=True)\r\n",
        "\r\n",
        "# Define the date range for filtering\r\n",
        "start_date = pd.to_datetime('2023-01-11')\r\n",
        "end_date = pd.to_datetime('2024-10-30')\r\n",
        "\r\n",
        "# Filter the DataFrames for the specified date range\r\n",
        "df_appointment_TX = df_appointment_TX[(df_appointment_TX['Month'] >= start_date) & (df_appointment_TX['Month'] <= end_date)]\r\n",
        "df_appointment_GA = df_appointment_GA[(df_appointment_GA['Month'] >= start_date) & (df_appointment_GA['Month'] <= end_date)]\r\n",
        "df_appointment_NY = df_appointment_NY[(df_appointment_NY['Month'] >= start_date) & (df_appointment_NY['Month'] <= end_date)]\r\n",
        "\r\n",
        "# Filter for appointments with Reason = \"Headache Treatment\"\r\n",
        "df_health_screening = df_appointments.filter(df_appointments.Reason == \"Headache Treatment\") \\\r\n",
        "    .groupBy(\"Month\") \\\r\n",
        "    .agg(F.count(\"AppointmentID\").alias(\"AppointmentReasons\")) \\\r\n",
        "    .toPandas()\r\n",
        "\r\n",
        "# Convert Month to datetime and ensure it's sorted\r\n",
        "df_health_screening['Month'] = pd.to_datetime(df_health_screening['Month'])\r\n",
        "df_health_screening = df_health_screening.sort_values(by=\"Month\", ascending=True)\r\n",
        "\r\n",
        "# Create a combined plot with line plots and histogram\r\n",
        "fig, ax1 = plt.subplots(figsize=(10, 5))\r\n",
        "\r\n",
        "# Line plots for TX, GA, and NY\r\n",
        "ax1.plot(df_appointment_TX['Month'], df_appointment_TX['TotalAppointments'], marker='o', color='blue', label='TX')\r\n",
        "ax1.plot(df_appointment_GA['Month'], df_appointment_GA['TotalAppointments'], marker='o', color='green', label='GA')\r\n",
        "ax1.plot(df_appointment_NY['Month'], df_appointment_NY['TotalAppointments'], marker='o', color='red', label='NY')\r\n",
        "\r\n",
        "# Labels and title for the line plots\r\n",
        "ax1.set_xlabel('Month')\r\n",
        "ax1.set_ylabel('TotalAppointments')\r\n",
        "ax1.set_title('Appointments and Health Screenings Over Time')\r\n",
        "ax1.legend(loc='upper left')\r\n",
        "ax1.tick_params(axis='x', rotation=45)\r\n",
        "\r\n",
        "# Histogram for Health Screening appointments\r\n",
        "ax2 = ax1.twinx()\r\n",
        "ax2.bar(df_health_screening['Month'], df_health_screening['AppointmentReasons'], color='orange', alpha=0.3, width=15, label='Injury Treatment')\r\n",
        "\r\n",
        "# Labels and title for the histogram\r\n",
        "ax2.set_ylabel('Headache Treatment Appointments ')\r\n",
        "ax2.legend(loc='upper right')\r\n",
        "plt.tight_layout()\r\n",
        "\r\n",
        "plt.show()\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Save transformed data with Source column added to the raw file\r\n",
        "\r\n",
        "df_patients.write.mode(\"overwrite\").parquet(f\"{base_path}ehr/silver/refined/Patients\")\r\n",
        "df_doctors.write.mode(\"overwrite\").parquet(f\"{base_path}ehr/silver/refined/Doctors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### save Dataframe in a parquet format with Patitionning \r\n",
        "Let's update your code to partition the data by Year and Month using AppointmentDate for df_appointments and recordDate for df_medrecords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "# Add Year and Month columns for partitioning\r\n",
        "df_appointments = df_appointments.withColumn(\"Year\", F.year(F.col(\"AppointmentDate\"))) \\\r\n",
        "                                 .withColumn(\"Month\", F.month(F.col(\"AppointmentDate\")))\r\n",
        "\r\n",
        "df_medrecords = df_medrecords.withColumn(\"Year\", F.year(F.col(\"recordDate\"))) \\\r\n",
        "                             .withColumn(\"Month\", F.month(F.col(\"recordDate\")))\r\n",
        "\r\n",
        "# Write the data to parquet files with partitioning by Year and Month\r\n",
        "df_appointments.write.mode(\"overwrite\").partitionBy(\"Year\", \"Month\").parquet(f\"{base_path}ehr/silver/refined/Appointments\")\r\n",
        "df_medrecords.write.mode(\"overwrite\").partitionBy(\"Year\", \"Month\").parquet(f\"{base_path}ehr/silver/refined/MedicalRecords\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Create Delta Lake tables\r\n",
        "Delta lake is built on tables, which provide a relational storage abstraction over files in a data lake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "delta_path = f\"{base_path}ehr/silver/delta/\"\r\n",
        "\r\n",
        "df_patients.write.mode(\"overwrite\").format(\"delta\").save(f\"{delta_path}Patients\")\r\n",
        "df_doctors.write.mode(\"overwrite\").format(\"delta\").save(f\"{delta_path}Doctors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Let's update the code to partition the data by Year and Month using AppointmentDate for df_appointments and recordDate for df_medrecords and save it in the delta format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Write the data to delta Format with partitioning by Year and Month\r\n",
        "\r\n",
        "df_appointments.write.mode(\"overwrite\").partitionBy(\"Year\", \"Month\").format(\"delta\").save(f\"{delta_path}Appointments\")\r\n",
        "df_medrecords.write.mode(\"overwrite\").partitionBy(\"Year\", \"Month\").format(\"delta\").save(f\"{delta_path}MedicalRecords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Import necessary libraries:\r\n",
        "\r\n",
        "from delta.tables import *\r\n",
        "from pyspark.sql.functions import col, lit\r\n",
        "\r\n",
        "# Load the Patients Delta table\r\n",
        "\r\n",
        "delta_table = DeltaTable.forPath(spark, f\"{delta_path}Patients\")\r\n",
        "\r\n",
        "# Perform the update\r\n",
        "\r\n",
        "delta_table.update(\r\n",
        "    condition = (col(\"PatientID\") == 4)&(col(\"Source\") == \"TX\"),\r\n",
        "    set = {\r\n",
        "        \"FirstName\": lit(\"Michel\"),\r\n",
        "        \"LastName\": lit(\"Agah\"),\r\n",
        "        \"PhoneNumber\": lit(\"1234567891\")\r\n",
        "    }\r\n",
        ")\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from delta.tables import *\r\n",
        "from pyspark.sql.functions import col, lit\r\n",
        "\r\n",
        "\r\n",
        "# Load the Delta table\r\n",
        "delta_table = DeltaTable.forPath(spark, f\"{delta_path}Patients\")\r\n",
        "\r\n",
        "# Update the record where DateOfbirth is 1954-02-28 and LastName is \"Travis\"\r\n",
        "delta_table.update(\r\n",
        "    condition = (col(\"DateOfbirth\") == \"1954-02-28\") & (col(\"LastName\") == \"Travis\")&(col(\"Source\") == \"TX\"),\r\n",
        "    set = {\r\n",
        "        \"PhoneNumber\": lit(\"1234567892\")\r\n",
        "    }\r\n",
        ")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "See the data before the first changes on the frist and last name and Phone Number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_before_first_change = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(f\"{delta_path}Patients\")\r\n",
        "df_before_first_change.filter(df_before_first_change.PatientID==10).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_before_first_change = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-11-02 03:18:37.278\").load(f\"{delta_path}Patients\")\r\n",
        "df_before_first_change.filter(df_before_first_change.PatientID==10).show()\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "See the data before the second changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_before_first_change = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(f\"{delta_path}Patients\")\r\n",
        "df_before_first_change.filter(df_before_first_change.PatientID==10).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_before_first_change = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-11-02 03:19:37.278\").load(f\"{delta_path}Patients\")\r\n",
        "df_before_first_change.filter((df_before_first_change.LastName==\"Agah\")&(df_before_first_change.Source==\"TX\")).show()\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "how to create new record in your Delta table\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "delta_table = DeltaTable.forPath(spark, f\"{delta_path}Patients\")\r\n",
        "delta_table.toDF().printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.functions import lit, to_date\r\n",
        "from pyspark.sql.types import IntegerType\r\n",
        "\r\n",
        "# Create a DataFrame with the new patient data\r\n",
        "\r\n",
        "columns = [\"PatientID\",\"FirstName\", \"LastName\", \"DateOfBirth\",\"Gender\",\"PhoneNumber\",\"Email\",\"Source\"]\r\n",
        "new_patient_data = [(99001,\"Christelle\", \"Williams\",\"1985-11-29\",\"F\",\"240.321.6352\",\"christelle.williams@example.com\",\"ON\")]\r\n",
        "\r\n",
        "new_patient_df = spark.createDataFrame(new_patient_data, columns)\r\n",
        "new_patient_df = new_patient_df.withColumn(\"PatientID\", new_patient_df[\"PatientID\"].cast(IntegerType()))\r\n",
        "new_patient_df = new_patient_df.withColumn(\"DateOfBirth\", to_date(new_patient_df[\"DateOfBirth\"], \"yyyy-MM-dd\"))\r\n",
        "\r\n",
        "# Append the new data to the Delta table\r\n",
        "new_patient_df.write.format(\"delta\").mode(\"append\").save(f\"{delta_path}Patients\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Query the Delta table\r\n",
        "\r\n",
        "df = spark.read.format(\"delta\").load(f\"{delta_path}Patients\")\r\n",
        "df.filter(df.PatientID==99001).show()\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Create an External catalog tables\r\n",
        "Data is stored at the specified location. Only metadata is deleted; data files remain at delta_catalog_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "delta_catalog_path = f\"{base_path}ehr/silver/catalog/\"\r\n",
        "\r\n",
        "df.write.format(\"delta\").option(\"path\",f\"{delta_catalog_path}Patients\").saveAsTable(\"ECT_DimPatients\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Create a Managed catalog tables\r\n",
        "Data is stored in Spark’s storage. Both metadata and data files are deleted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "you can use Spark.SQL or SQL Directly by leveraging the magic sign %%sql."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "outputs": [],
      "metadata": {
        "microsoft": {},
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(\"CREATE TABLE MCT_DimPatients (id INT, FirstName STRING,LastName STRING)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        },
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "CREATE TABLE MCT_FactAppointments\r\n",
        "(\r\n",
        "    AppointmentID INT NOT NULL,\r\n",
        "    PatientID INT NOT NULL,\r\n",
        "    DoctorID INT NOT NULL,\r\n",
        "    AppointmentDate TIMESTAMP NOT NULL,\r\n",
        "    Reason STRING,\r\n",
        "    Source STRING\r\n",
        ")\r\n",
        "USING DELTA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Dropping the table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(\"DROP TABLE MN_DimPatients\")\r\n",
        "spark.sql(\"DROP TABLE MN_FactAppointments\")\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": true,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "import time\r\n",
        "\r\n",
        "\r\n",
        "# Create a simple DataFrame\r\n",
        "df = spark.range(1)\r\n",
        "\r\n",
        "# Write a loop to keep the cluster alive for 30 minutes\r\n",
        "for _ in range(30):\r\n",
        "    df.show()  # Perform a simple action to keep the cluster active\r\n",
        "    time.sleep(60)  # Sleep for 1 minute\r\n",
        "\r\n",
        "# Stop the Spark session\r\n",
        "\r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}